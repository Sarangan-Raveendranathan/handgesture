# -*- coding: utf-8 -*-
"""Handgesture

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r027NPEcKdIgjydG7lDH2mIeu8lvhJ0k
"""

# prompt: extract from the uploaded zipped folder, train_data.zip.

import os
import zipfile

# Unzip the uploaded zip file.
with zipfile.ZipFile("train_data.zip", "r") as zip_ref:
    zip_ref.extractall("train_data")

# Change the current working directory to the extracted folder.
os.chdir("train_data")

import torch
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, random_split
import matplotlib.pyplot as plt
import numpy as np

# Define transformations
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize images to a fixed size
    transforms.Grayscale(num_output_channels=1),  # Convert images to grayscale
    transforms.ToTensor()  # Convert images to tensors
])

# Create dataset
dataset = datasets.ImageFolder(root='/content/train_data/train_data', transform=transform)

# Split dataset into train and test sets
train_size = int(0.8 * len(dataset))
test_size = len(dataset) - train_size
train_dataset, test_dataset = random_split(dataset, [train_size, test_size])

# Define data loaders for train and test sets
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)

# Define CNN model
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, kernel_size=3, padding=1)
        self.relu1 = nn.ReLU()
        self.pool1 = nn.MaxPool2d(kernel_size=2)
        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)
        self.relu2 = nn.ReLU()
        self.pool2 = nn.MaxPool2d(kernel_size=2)
        self.flatten = nn.Flatten()
        self.fc = nn.Linear(32 * 56 * 56, 5)  # Adjust 32 * 56 * 56 according to the size of your images

    def forward(self, x):
        x = self.pool1(self.relu1(self.conv1(x)))
        x = self.pool2(self.relu2(self.conv2(x)))
        x = self.flatten(x)
        x = self.fc(x)
        return x

model = CNN()

# Define loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
num_epochs = 10
for epoch in range(num_epochs):
    model.train()  # Set model to training mode
    for images, labels in train_loader:
        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

    print(f'Training - Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')

    # Validation loop
    model.eval()  # Set model to evaluation mode
    with torch.no_grad():
        correct = 0
        total = 0
        predicted_images = []
        for images, labels in test_loader:
            outputs = model(images)
            _, predicted = torch.max(outputs, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            # Collect images and predictions
            if len(predicted_images) < 9:  # Collect only 9 images
                image = images.squeeze().numpy()
                predicted_images.append((image, predicted.item(), labels.item()))

    accuracy = 100 * correct / total
    print(f'Validation - Epoch [{epoch+1}/{num_epochs}], Accuracy: {accuracy:.2f}%')

# Display the collected images and predictions
fig, axs = plt.subplots(3, 3, figsize=(10, 10))
fig.suptitle('Predicted vs Actual Labels', fontsize=16)
for i, (image, predicted_label, actual_label) in enumerate(predicted_images):
    ax = axs[i // 3, i % 3]
    ax.imshow(image, cmap='gray')
    ax.set_title(f'Predicted: {predicted_label}, Actual: {actual_label}')
    ax.axis('off')

plt.show()

# Save the trained model
torch.save(model.state_dict(), 'trained_model.pth')

torch.save(model.state_dict(), )



